# CoMViT

**CoMViT** (Compact Medical Vision Transformer) is a lightweight and efficient Vision Transformer designed for low-resource medical imaging tasks. It is size-configured for optimization and replaces rigid patch tokenization with optimized convolutional tokenizers, incorporates learnable sequence pooling, and applies locality-aware self-attention through diagonal masking and learnable temperature scaling techniques.

---

## ğŸ”¬ Highlights

- ğŸ§  Optimized for **medical image classification** under resource constraints
- âš™ï¸ Uses **convolutional tokenization** instead of patch splitting
- ğŸ§© Sequence pooling instead of CLS token
- ğŸ” Locality Self-Attention (LSA) for focused attention
- ğŸ“¦ ~4.5M parameters â€” smaller than most ViTs and CNNs

---

## ğŸ“ Directory Structure
```bash
Compact-Transformers/
â”œâ”€â”€ configs/                 # Dataset-specific YAML configs
â”œâ”€â”€ src/                    # Core model code (backbones, pooling, tokenizer)
â”œâ”€â”€ train.py                # Main training script
â”œâ”€â”€ main_medmnist.py        # MedMNIST-specific training entry point
â”œâ”€â”€ solver.py               # Loss and optimizer configuration
â”œâ”€â”€ dist_train.sh           # Slurm-based distributed training launcher
â”œâ”€â”€ get_flops.py            # FLOPs and parameter calculation
â”œâ”€â”€ Variants.md             # Description of tested model variants
â”œâ”€â”€ output/                 # Saved checkpoints and logs
```

---

## ğŸš€ Getting Started

### 1. Clone the Repository
```bash
git clone https://github.com/aonsafdar/CoMViT.git
cd CoMViT/Compact-Transformers
```

### 2. Install Dependencies
We recommend using conda:
```bash
conda create -n comvit python=3.9
conda activate comvit
pip install -r requirements.txt
```

You may also need:
```bash
pip install timm medmnist torch torchvision
```

---

## ğŸ‹ï¸â€â™‚ï¸ Training

To train `cct_7_7x2_224` on a MedMNIST dataset (e.g., PathMNIST):

```bash
python train.py \
  --model cct_7_7x2_224 \
  --dataset_name PathMNIST \
  --dataset_path ./datasets/medmnist \
  --img_size 224 \
  --batch_size 128 \
  --epochs 100 \
  --lr 5e-4 \
  --output_dir ./output
```

For training all datasets:
```bash
python main_medmnist.py --model cct_7_7x2_224
```

---

## ğŸ§ª Evaluation
To evaluate a saved checkpoint:
```bash
python train.py \
  --eval \
  --resume ./output/best_checkpoint.pth \
  --dataset_name PathMNIST
```

---

## ğŸ§° Configuration Files
You can pass a YAML file for dataset-specific settings. Example:
```yaml
# configs/pathmnist.yaml
dataset: pathmnist
num_classes: 9
img_size: 224
mean: [0.5, 0.5, 0.5]
std: [0.5, 0.5, 0.5]
batch_size: 128
lr: 5e-4
model: cct_7_7x2_224
```
Then run:
```bash
python train.py --config configs/pathmnist.yaml
```

---

## ğŸ“Š Citation
If you use CoMViT in your research, please cite:
```bibtex
@inproceedings{comvit2025,
  title     = {CoMViT: Efficient Vision Backbone for Representation Learning in Medical Imaging},
  author    = {xyz},
  booktitle = {MICCAI MIRASOL Workshop},
  year      = {2025}
}
```

---

## ğŸ”— Related Projects
- [CCT (Official)](https://github.com/SHI-Labs/Compact-Transformers)
- [MedMNIST](https://medmnist.com)

---

## ğŸ“¬ Contact
For questions or contributions, please contact xyz

---

MIT License. See [LICENSE](../LICENSE).
